{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMMoKz4iPbh2"
   },
   "source": [
    "This code generates **Figure 3 and 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cx46A8G8arzN"
   },
   "outputs": [],
   "source": [
    "## Required Liberaries: \n",
    "#! pip3 install torch \n",
    "#! pip3 install torchvision\n",
    "# and GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4_Ihh6BEOSxd"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M745w4CofR-O"
   },
   "source": [
    "Loading cifar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": [
      "a2a7156bf8234d028db073003ccd9ff2",
      "83722ebd26984bbfb4fa6d675921b193",
      "7e94fd378fdb4a1fa05d91ba655cfc99",
      "bda069fe05b24f75a0709af8baff7ed0",
      "fbeb6fba1eb144fcb5816d6d907f2aad",
      "b70fa52b38a745859bdf7361c2695d20",
      "fcec701a07974b0c82b530ded44aaceb",
      "df9a07d33fce4f6e958c8ce4706d179a"
     ]
    },
    "id": "GIB-Yw_jOSxk",
    "outputId": "00803f24-62ea-4252-d92d-d0e48cfb6f71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size_train = 500\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.CIFAR10('./data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8Fcfh-eIRygo"
   },
   "outputs": [],
   "source": [
    "# checking the hardware\n",
    "# !nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dPu2ItfLOSxk"
   },
   "outputs": [],
   "source": [
    "### tensor types, to run the code on CPU please update the types \n",
    "dtype = torch.cuda.FloatTensor # CPU -> torch.FloatTensor\n",
    "dtype_labels = torch.cuda.LongTensor # CPU -> torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJDe-AHUcg4d"
   },
   "source": [
    "Implementation of a multiLayer prectron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TKy2O62cOSxl"
   },
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module): \n",
    "    def __init__(self,layer_num,input_size,width,out_size,bias_on=False,act = None): # act is the activation function\n",
    "        super().__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.width = width\n",
    "        self.out_size = out_size\n",
    "        self.bias_on = bias_on\n",
    "        self.act = act\n",
    "        self.layers = torch.nn.ModuleList() # contains linear layers \n",
    "        self.layers.append(torch.nn.Linear(input_size,width,bias=bias_on))\n",
    "        for i in range(layer_num-2): \n",
    "              self.layers.append(torch.nn.Linear(width,width,bias=bias_on))\n",
    "        self.layers.append(torch.nn.Linear(width,out_size,bias=bias_on))\n",
    "        self.act = act\n",
    "    def forward(self,x): \n",
    "        out = x\n",
    "        index = 0\n",
    "        for lay in self.layers: # passing input through the layers \n",
    "            index = index +1 \n",
    "            if self.act is not None and index<self.layer_num: \n",
    "                out = self.act(lay(out))\n",
    "            else: \n",
    "                out = lay(out)\n",
    "        return out\n",
    "    def forward_minus(self,x): # skip the output layer\n",
    "        out = x\n",
    "        index = 0\n",
    "        for lay in self.layers:\n",
    "            index = index +1 \n",
    "            if self.act is not None and index<self.layer_num-1: \n",
    "                out = self.act(lay(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKDWsppecn9c"
   },
   "source": [
    "\n",
    "MLP with batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3h_AkIXccnJT"
   },
   "outputs": [],
   "source": [
    "class BNMLP(torch.nn.Module): \n",
    "    def __init__(self,layer_num,input_size,width,out_size,bias_on=False,act = None): # act is the activation function\n",
    "        super().__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.width = width\n",
    "        self.out_size = out_size\n",
    "        self.bias_on = bias_on\n",
    "        self.act = act\n",
    "        self.layers = torch.nn.ModuleList() # linear layers\n",
    "        self.layers.append(torch.nn.Linear(input_size,width,bias=bias_on))\n",
    "        for i in range(layer_num-2): \n",
    "              self.layers.append(torch.nn.Linear(width,width,bias=bias_on))\n",
    "        self.layers.append(torch.nn.Linear(width,out_size,bias=bias_on))\n",
    "        self.act = act\n",
    "        self.bns = torch.nn.ModuleList() # batch normalization layers\n",
    "        for i in range(layer_num-1): \n",
    "              self.bns.append(torch.nn.BatchNorm1d(num_features=width)) # for MLPs we use 1d batch normalization\n",
    "    def forward(self,x): \n",
    "        out = x\n",
    "        for i in range(self.layer_num-1):\n",
    "            if self.act is not None: \n",
    "                out = self.act(self.bns[i](self.layers[i](out)))\n",
    "            else: \n",
    "                out = self.bns[i](self.layers[i](out))\n",
    "       \n",
    "        out = self.layers[self.layer_num-1](out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "667Q9IWycugh"
   },
   "source": [
    "Here, we implement xavier's initialization of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8tOv3c55OSxl"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-19caada38569>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mxavier_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# xavier initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "layer_num = 50\n",
    "out_size =10 \n",
    "input_size = 3*32*32\n",
    "width = batch_size_train\n",
    "net = MLP(layer_num, input_size,width,out_size)\n",
    "net = net.cuda()\n",
    "def xavier_init(m): # xavier initialization \n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "def kaiming_init(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5NpDsvbarBf"
   },
   "source": [
    "The following functions implements the iterative initialization. Let $H_\\ell \\in R^{d \\times n}$ denotes hidden representations in layer $\\ell$. we use SVD decompostion $H_\\ell = U_\\ell \\Sigma_\\ell V_\\ell^\\top$ to initialize weights in layer $\\ell$ as \n",
    "$$W_\\ell = \\frac{1}{\\| \\Sigma^{1/2} \\|_F } V'_\\ell \\Sigma_\\ell^{1/2} U_\\ell^\\top$$ \n",
    "where $V'_\\ell$ is a slice of $V_\\ell$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nG4FevBCOSxm"
   },
   "outputs": [],
   "source": [
    "def improved_init(net, width):\n",
    "   # picking a large mini batch of inputs\n",
    "    batch_size_train = 3*width\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "      torchvision.datasets.CIFAR10('./data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "                             ])),\n",
    "     batch_size=batch_size_train, shuffle=True)\n",
    "    examples = enumerate(loader)\n",
    "    batch_idx, (images, example_targets) = next(examples)\n",
    "    H0 = images.view(-1,3*32*32).type(dtype)\n",
    "   \n",
    "    H = H0\n",
    "    gamma = 0.1\n",
    "    layer_num = net.layer_num\n",
    "    # passing the input and compute the hidden representation and weights iteratively  \n",
    "    for i in range(layer_num-1):\n",
    "        if i>0: \n",
    "            Hdata = H.data\n",
    "            u,s, v = torch.svd(Hdata) # svd computation \n",
    "            wd = net.layers[i].weight.data.size(0)\n",
    "            w = u[0:wd,0:wd].mm(torch.diag(1/torch.pow(s[0:wd],1))).mm(v.t()[0:wd,:]) # weight initialization\n",
    "            net.layers[i].weight.data = w\n",
    "            net.layers[i].weight.data = w \n",
    "            net.layers[i].weight.data = net.layers[i].weight.data/torch.norm(net.layers[i](H)) # normalization factor\n",
    "        if net.act is not None:\n",
    "            H = net.act(net.layers[i](H))\n",
    "        else:\n",
    "            H = net.layers[i](H)\n",
    "   \n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JJZoiWTue92"
   },
   "outputs": [],
   "source": [
    "def compute_orthogonality_gap(net):\n",
    "  loader = torch.utils.data.DataLoader(\n",
    "      torchvision.datasets.CIFAR10('./data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "                             ])),\n",
    "     batch_size=batch_size_train, shuffle=True)\n",
    "  avg_gap = 0\n",
    "  index = 0 \n",
    "  for x,y in loader: \n",
    "    index += 1\n",
    "    x = x.type(dtype)\n",
    "    y = y.type(dtype_labels)\n",
    "    out = net.forward(torch.flatten(x,1))\n",
    "    u1,s1,v1 = torch.svd(out.data)\n",
    "    s1 = s1/torch.norm(s1)\n",
    "    s2 = torch.tensor(np.ones(s1.size(0))/math.sqrt(s1.size(0))).type(dtype)\n",
    "    avg_gap += torch.norm(s1-s2)\n",
    "  avg_gap = avg_gap/index\n",
    "  print('average of the orthogonality gap is ',avg_gap)\n",
    "  return avg_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmp297Vt8B4s"
   },
   "outputs": [],
   "source": [
    "def train(net,train_loader,epochs_num = 10,stepsize=0.01):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(),lr=stepsize)\n",
    "    conv = []\n",
    "    gaps = []\n",
    "    itrs = []\n",
    "    for i in range(epochs_num):\n",
    "        current_loss = 0 \n",
    "        r_size = 0\n",
    "        for x,y in train_loader: \n",
    "            x = x.type(dtype)\n",
    "            y = y.type(dtype_labels)\n",
    "            r_size += x.size(0)\n",
    "            optimizer.zero_grad()\n",
    "            predy = net(torch.flatten(x,1))\n",
    "            loss = loss_function(predy,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            current_loss += loss.item()*batch_size_train\n",
    "        if i % 5 == 0:\n",
    "          gaps.append(compute_orthogonality_gap(net))   \n",
    "          conv.append(current_loss/r_size)\n",
    "          itrs.append(i+1)\n",
    "          print(current_loss/r_size) \n",
    "    return conv,gaps,itrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLk-Gv2woS8S"
   },
   "outputs": [],
   "source": [
    "# computing the decay in the orthogonality gap for the iterative initialization ## in figure 3.b \n",
    "ACTIVE = torch.nn.functional.relu\n",
    "convs =[] # convergence rate\n",
    "gaps = [] # orthogonality gap during training\n",
    "repeat = 5\n",
    "for i in range(repeat):\n",
    "  mynet = MLP(20, input_size,1000,out_size,act=torch.nn.functional.relu,bias_on = False)\n",
    "  mynet = mynet.cuda()\n",
    "  mynet.apply(xavier_init)\n",
    "  conv, gap,itrs = train(mynet,train_loader,epochs_num = 50)\n",
    "  convs.append(conv)\n",
    "  gaps.append(gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBptIV3EjdPN"
   },
   "source": [
    "The convergence for different initialization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nF9PijG9iVY"
   },
   "outputs": [],
   "source": [
    "# Figures 3.a, 4.a, and 4.b\n",
    "layers = [15,30,45,60,75]\n",
    "bias_on = False\n",
    "width = 800\n",
    "epochs = 30\n",
    "repeat = 4\n",
    "ACTIVE = torch.nn.functional.relu\n",
    "ouresults = np.zeros((len(layers),epochs,repeat))\n",
    "xavir_result = np.zeros((len(layers),epochs,repeat))\n",
    "for j in range(repeat):\n",
    "  for i in range(len(layers)): \n",
    "      layer_num = layers[i]\n",
    "      mynet = MLP(layer_num, input_size,width,out_size,act=ACTIVE,bias_on = bias_on)\n",
    "      mynet = mynet.cuda()\n",
    "      mynet.apply(xavier_init)\n",
    "      mynet = improved_init(mynet,width)\n",
    "      print(i,'our initialization -----')\n",
    "      res = train(mynet,train_loader,epochs_num = epochs)\n",
    "      ouresults[i,:,j] = res[0]\n",
    "      print(i,'xavier initialization----')\n",
    "      netx = MLP(layer_num, input_size,width,out_size,act=ACTIVE,bias_on = bias_on)\n",
    "      netx = netx.cuda()\n",
    "      netx = netx.apply(xavier_init)\n",
    "      result_xavier =  train(netx,train_loader,epochs_num = epochs)\n",
    "      xavir_result[i,:,j] =result_xavier[0]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BN_NIPS21_Figures_3_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "7e94fd378fdb4a1fa05d91ba655cfc99": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b70fa52b38a745859bdf7361c2695d20",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fbeb6fba1eb144fcb5816d6d907f2aad",
      "value": 170498071
     }
    },
    "83722ebd26984bbfb4fa6d675921b193": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2a7156bf8234d028db073003ccd9ff2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7e94fd378fdb4a1fa05d91ba655cfc99",
       "IPY_MODEL_bda069fe05b24f75a0709af8baff7ed0"
      ],
      "layout": "IPY_MODEL_83722ebd26984bbfb4fa6d675921b193"
     }
    },
    "b70fa52b38a745859bdf7361c2695d20": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bda069fe05b24f75a0709af8baff7ed0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df9a07d33fce4f6e958c8ce4706d179a",
      "placeholder": "​",
      "style": "IPY_MODEL_fcec701a07974b0c82b530ded44aaceb",
      "value": " 170499072/? [10:33&lt;00:00, 269158.70it/s]"
     }
    },
    "df9a07d33fce4f6e958c8ce4706d179a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbeb6fba1eb144fcb5816d6d907f2aad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fcec701a07974b0c82b530ded44aaceb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
